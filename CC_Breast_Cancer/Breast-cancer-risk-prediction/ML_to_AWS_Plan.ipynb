{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749df35b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70cc63e6",
   "metadata": {},
   "source": [
    "This is a solid architecture for a lightweight ML application. Migrating from Jupyter Notebooks to a cloud architecture involves moving from \"interactive exploration\" to \"stateless scripts.\"\n",
    "\n",
    "Here is your step-by-step migration plan, mapping your specific notebooks to the AWS components in your diagram.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 1: Prepare the Model Code (The \"EC2\" Part)\n",
    "\n",
    "Your diagram shows an **EC2 instance** used for \"Model Training.\" You currently have 6 notebooks. You don't need all of them for production. You only need the **best performing pipeline**.\n",
    "\n",
    "Based on your notebooks, **NB6 (Comparison)** and **NB5 (Optimizing SVM)** contain the final logic.\n",
    "\n",
    "**1. Consolidate Logic into `train.py`**\n",
    "You need to create a single Python script (`train.py`) that runs on the EC2 instance. This script will replace the manual running of notebooks.\n",
    "\n",
    "*   **Input:** `data/clean-data.csv` (or raw data).\n",
    "*   **Logic to extract:**\n",
    "    *   **From NB3/NB6:** The `Pipeline` creation. *Crucial:* You must use a Pipeline (StandardScaler + PCA + SVC) so that raw input data is automatically scaled and transformed during prediction.\n",
    "    *   **From NB5:** The best hyperparameters you found (e.g., `C`, `gamma`, `kernel`).\n",
    "*   **Output:** A serialized file (`model_v1.pkl`).\n",
    "\n",
    "**Code Snippet for `train.py` (to run on EC2):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3010c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "data/clean-data.csv not found locally and S3 download failed: Unable to locate credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoCredentialsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 19\u001b[0m\n\u001b[0;32m     18\u001b[0m s3_tmp \u001b[38;5;241m=\u001b[39m boto3\u001b[38;5;241m.\u001b[39mclient(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms3\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[43ms3_tmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource_bucket\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDATA_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from s3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_bucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m     hook()\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\boto3\\s3\\inject.py:223\u001b[0m, in \u001b[0;36mdownload_file\u001b[1;34m(self, Bucket, Key, Filename, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtransfer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBucket\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mKey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mExtraArgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\boto3\\s3\\transfer.py:406\u001b[0m, in \u001b[0;36mS3Transfer.download_file\u001b[1;34m(self, bucket, key, filename, extra_args, callback)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 406\u001b[0m     \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[38;5;66;03m# This is for backwards compatibility where when retries are\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# exceeded we need to throw the same error from boto3 instead of\u001b[39;00m\n\u001b[0;32m    409\u001b[0m \u001b[38;5;66;03m# s3transfer's built in RetriesExceededError as current users are\u001b[39;00m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;66;03m# catching the boto3 one instead of the s3transfer exception to do\u001b[39;00m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;66;03m# their own retries.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\s3transfer\\futures.py:111\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_coordinator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\s3transfer\\futures.py:287\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m--> 287\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\s3transfer\\tasks.py:272\u001b[0m, in \u001b[0;36mSubmissionTask._main\u001b[1;34m(self, transfer_future, **kwargs)\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;66;03m# Call the submit method to start submitting tasks to execute the\u001b[39;00m\n\u001b[0;32m    271\u001b[0m     \u001b[38;5;66;03m# transfer.\u001b[39;00m\n\u001b[1;32m--> 272\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_submit(transfer_future\u001b[38;5;241m=\u001b[39mtransfer_future, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    273\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;66;03m# If there was an exception raised during the submission of task\u001b[39;00m\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;66;03m# there is a chance that the final task that signals if a transfer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    284\u001b[0m \n\u001b[0;32m    285\u001b[0m     \u001b[38;5;66;03m# Set the exception, that caused the process to fail.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\s3transfer\\download.py:355\u001b[0m, in \u001b[0;36mDownloadSubmissionTask._submit\u001b[1;34m(self, client, config, osutil, request_executor, io_executor, transfer_future, bandwidth_limiter)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    352\u001b[0m     transfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39msize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    353\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m transfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39metag \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    354\u001b[0m ):\n\u001b[1;32m--> 355\u001b[0m     response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mhead_object(\n\u001b[0;32m    356\u001b[0m         Bucket\u001b[38;5;241m=\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mbucket,\n\u001b[0;32m    357\u001b[0m         Key\u001b[38;5;241m=\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mkey,\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtransfer_future\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mcall_args\u001b[38;5;241m.\u001b[39mextra_args,\n\u001b[0;32m    359\u001b[0m     )\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;66;03m# If a size was not provided figure out the size for the\u001b[39;00m\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;66;03m# user.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\client.py:602\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    122\u001b[0m     hook()\n\u001b[1;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\client.py:1060\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1059\u001b[0m     apply_request_checksum(request_dict)\n\u001b[1;32m-> 1060\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1061\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_context\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[0;32m   1065\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1066\u001b[0m     http_response\u001b[38;5;241m=\u001b[39mhttp,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1069\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[0;32m   1070\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\client.py:1084\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[1;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1084\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_endpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1085\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[1;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[0;32m    114\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    116\u001b[0m     operation_model,\n\u001b[0;32m    117\u001b[0m     request_dict,\n\u001b[0;32m    118\u001b[0m )\n\u001b[1;32m--> 119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\endpoint.py:196\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[1;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[1;32m--> 196\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperation_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[0;32m    198\u001b[0m     request, operation_model, context\n\u001b[0;32m    199\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\endpoint.py:132\u001b[0m, in \u001b[0;36mEndpoint.create_request\u001b[1;34m(self, params, operation_model)\u001b[0m\n\u001b[0;32m    131\u001b[0m     event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest-created.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_emitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43memit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperation_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m prepared_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(request)\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\hooks.py:412\u001b[0m, in \u001b[0;36mEventAliaser.emit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    411\u001b[0m aliased_event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alias_event_name(event_name)\n\u001b[1;32m--> 412\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_emitter\u001b[38;5;241m.\u001b[39memit(aliased_event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\hooks.py:256\u001b[0m, in \u001b[0;36mHierarchicalEmitter.emit\u001b[1;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;124;03mEmit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[0;32m    248\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m         handlers.\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_emit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevent_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\hooks.py:239\u001b[0m, in \u001b[0;36mHierarchicalEmitter._emit\u001b[1;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[0;32m    238\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling handler \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, event_name, handler)\n\u001b[1;32m--> 239\u001b[0m response \u001b[38;5;241m=\u001b[39m handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    240\u001b[0m responses\u001b[38;5;241m.\u001b[39mappend((handler, response))\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\signers.py:114\u001b[0m, in \u001b[0;36mRequestSigner.handler\u001b[1;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhandler\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;66;03m# This is typically hooked up to the \"request-created\" event\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# from a client's event emitter.  When a new request is created\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# this method is invoked to sign the request.\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;66;03m# Don't call this method directly.\u001b[39;00m\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msign\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\signers.py:206\u001b[0m, in \u001b[0;36mRequestSigner.sign\u001b[1;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[0;32m    204\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m--> 206\u001b[0m \u001b[43mauth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_auth\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Jad Zoghaib\\Downloads\\micromamba\\envs\\breastcancer\\lib\\site-packages\\botocore\\auth.py:422\u001b[0m, in \u001b[0;36mSigV4Auth.add_auth\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 422\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoCredentialsError()\n\u001b[0;32m    423\u001b[0m datetime_now \u001b[38;5;241m=\u001b[39m get_current_datetime()\n",
      "\u001b[1;31mNoCredentialsError\u001b[0m: Unable to locate credentials",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from s3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_bucket\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 22\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found locally and S3 download failed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(DATA_PATH)\n\u001b[0;32m     24\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m31\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: data/clean-data.csv not found locally and S3 download failed: Unable to locate credentials"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import boto3\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Load Data (Ideally from S3, but locally on EC2 for now)\n",
    "DATA_PATH = 'data/clean-data.csv'\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    # Try downloading from S3 if available\n",
    "    source_bucket = 'YOUR_SOURCE_DATA_BUCKET_NAME'\n",
    "    source_key = 'clean-data.csv'\n",
    "    try:\n",
    "        s3_tmp = boto3.client('s3')\n",
    "        s3_tmp.download_file(source_bucket, source_key, DATA_PATH)\n",
    "        print(f\"Downloaded {DATA_PATH} from s3://{source_bucket}/{source_key}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not download from S3: {e}\")\n",
    "        # Ensure file exists or raise error\n",
    "        if not os.path.exists(DATA_PATH):\n",
    "             raise FileNotFoundError(f\"{DATA_PATH} not found locally and S3 download failed.\")\n",
    "\n",
    "data = pd.read_csv(DATA_PATH)\n",
    "\n",
    "# --- Data Cleaning & Preprocessing (Replicating NB1 & NB3) ---\n",
    "# 1. Drop 'Unnamed: 0' (Artifact from saving CSV with index)\n",
    "if 'Unnamed: 0' in data.columns:\n",
    "    data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# 2. Drop 'id' if it exists (Redundant - from NB1)\n",
    "if 'id' in data.columns:\n",
    "    data.drop('id', axis=1, inplace=True)\n",
    "\n",
    "# 3. Drop 'Unnamed: 32' if it exists (Common dataset artifact)\n",
    "if 'Unnamed: 32' in data.columns:\n",
    "    data.drop('Unnamed: 32', axis=1, inplace=True)\n",
    "\n",
    "# 4. Separate Features and Target\n",
    "# After cleaning, 'diagnosis' is the first column (index 0)\n",
    "X = data.iloc[:, 1:].values # Features (columns 1 to end)\n",
    "y = data.iloc[:, 0].values  # Target (column 0)\n",
    "\n",
    "# 5. Encode Target (M=1, B=0) - from NB3\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# 2. Define the Pipeline (Logic from NB6)\n",
    "# Use the best params found in NB5\n",
    "pipeline = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    ('clf', SVC(C=100.0, kernel='rbf', probability=True)) \n",
    "])\n",
    "\n",
    "# 3. Train\n",
    "pipeline.fit(X, y)\n",
    "\n",
    "# 4. Save Model locally\n",
    "joblib.dump(pipeline, 'model_v1.pkl')\n",
    "\n",
    "# 5. Upload to S3 (The \"Stores Model\" bucket in your diagram)\n",
    "try:\n",
    "    s3 = boto3.client('s3')\n",
    "    s3.upload_file('model_v1.pkl', 'YOUR_MODEL_BUCKET_NAME', 'latest_model.pkl')\n",
    "    print(\"Model uploaded to S3\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not upload to S3 (Check credentials/bucket name): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ec6caa",
   "metadata": {},
   "source": [
    "### Phase 1.5: Environment Setup on EC2\n",
    "\n",
    "You mentioned you have a `.yaml` file ready. This is perfect. Here is the exact sequence of commands to set up your EC2 instance to run the `train.py` script.\n",
    "\n",
    "**1. Connect to your EC2 Instance**\n",
    "Use SSH or EC2 Instance Connect.\n",
    "\n",
    "**2. Install Micromamba (Faster than Conda)**\n",
    "Run these commands in your EC2 terminal to install the package manager:\n",
    "```bash\n",
    "\"${SHELL}\" <(curl -L micro.mamba.pm/install.sh)\n",
    "source ~/.bashrc\n",
    "```\n",
    "\n",
    "**3. Upload your Files**\n",
    "Upload `breastcancer_env.yaml` and `train.py` to the instance (you can use `scp` or VS Code Remote - SSH).\n",
    "\n",
    "**4. Create the Environment**\n",
    "```bash\n",
    "micromamba create -f breastcancer_env.yaml\n",
    "micromamba activate breastcancer\n",
    "```\n",
    "\n",
    "**5. Verify Dependencies**\n",
    "Since `boto3` (AWS SDK) is required for S3 access but might not be in your original data science YAML, ensure it is installed:\n",
    "```bash\n",
    "micromamba install boto3\n",
    "```\n",
    "\n",
    "**6. Run the Training Script**\n",
    "```bash\n",
    "python train.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab920e9a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Action Item:** Launch an EC2 instance (t3.medium is fine), install your environment using your `.yaml` file, run this script, and ensure `latest_model.pkl` appears in your S3 bucket.\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 2: The Predictor (The \"Lambda\" Part)\n",
    "\n",
    "This is the \"AWS Lambda: Predictor\" in your diagram. It needs to load the model from S3 and make a prediction.\n",
    "\n",
    "**Challenge:** AWS Lambda has size limits. `pandas` and `scikit-learn` are heavy.\n",
    "**Solution:** Use a **Lambda Container Image** (Docker) or use **AWS Data Wrangler Layers**.\n",
    "\n",
    "**Logic for `lambda_function.py`:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5350e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Initialize clients outside handler for caching\n",
    "s3 = boto3.client('s3')\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "sns = boto3.client('sns')\n",
    "table = dynamodb.Table('YOUR_DYNAMODB_TABLE_NAME')\n",
    "TOPIC_ARN = 'YOUR_SNS_TOPIC_ARN'\n",
    "\n",
    "# Download model to /tmp (Lambda's only writable storage)\n",
    "s3.download_file('YOUR_MODEL_BUCKET_NAME', 'latest_model.pkl', '/tmp/model.pkl')\n",
    "model = joblib.load('/tmp/model.pkl')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # 1. Parse Input (from API Gateway)\n",
    "    body = json.loads(event['body'])\n",
    "    # Expecting a list of 30 features\n",
    "    features = np.array(body['data']).reshape(1, -1)\n",
    "    \n",
    "    # 2. Predict\n",
    "    prediction = model.predict(features)[0] # 0 or 1\n",
    "    probability = model.predict_proba(features)[0].max()\n",
    "    \n",
    "    result = \"Malignant\" if prediction == 1 else \"Benign\"\n",
    "    \n",
    "    # 3. Store in DynamoDB\n",
    "    item = {\n",
    "        'patient_id': body.get('patient_id', 'unknown'),\n",
    "        'features': str(body['data']),\n",
    "        'prediction': result,\n",
    "        'confidence': str(probability),\n",
    "        'status': 'Pending Validation' # For your feedback loop\n",
    "    }\n",
    "    table.put_item(Item=item)\n",
    "    \n",
    "    # 4. SNS Notification (If Malignant)\n",
    "    if prediction == 1:\n",
    "        sns.publish(\n",
    "            TopicArn=TOPIC_ARN,\n",
    "            Message=f\"Alert: High risk patient detected. ID: {item['patient_id']}\",\n",
    "            Subject=\"Breast Cancer Risk Alert\"\n",
    "        )\n",
    "        \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps({'prediction': result, 'confidence': probability})\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8872cf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 3: The Retraining Loop (EventBridge + Lambda)\n",
    "\n",
    "This is the \"AWS Lambda: Retraining\" in your diagram.\n",
    "\n",
    "**How to adjust data location:**\n",
    "In your notebooks, you read from CSV. In this Lambda, you must read from **DynamoDB** (where you are storing new patient records) or a \"Historical Data\" folder in **S3**.\n",
    "\n",
    "**Logic:**\n",
    "1.  Triggered by EventBridge (e.g., every Sunday).\n",
    "2.  Fetch historical data + new \"Confirmed\" data from DynamoDB.\n",
    "3.  Combine them into a DataFrame.\n",
    "4.  Run the same training logic as Phase 1.\n",
    "5.  Overwrite `latest_model.pkl` in S3.\n",
    "\n",
    "*Note: If your dataset grows large, Lambda might time out (15 min limit). If that happens, you will need to move this logic back to EC2 or AWS SageMaker, but for < 10,000 rows, Lambda is fine.*\n",
    "\n",
    "---\n",
    "\n",
    "### Phase 4: Infrastructure Setup Checklist\n",
    "\n",
    "1.  **S3 Buckets:**\n",
    "    *   `bucket-models`: To store `latest_model.pkl`.\n",
    "    *   `bucket-website`: To host your HTML/JS frontend (if applicable).\n",
    "\n",
    "2.  **DynamoDB Table:**\n",
    "    *   Partition Key: `patient_id` (String).\n",
    "    *   Attributes: `prediction`, `features`, `actual_diagnosis` (for the feedback loop).\n",
    "\n",
    "3.  **SNS Topic:**\n",
    "    *   Create a topic.\n",
    "    *   Subscribe your email address to it.\n",
    "\n",
    "4.  **IAM Roles (The Glue):**\n",
    "    *   **EC2 Role:** Needs `S3FullAccess` (to upload the model).\n",
    "    *   **Lambda Role:** Needs `S3ReadOnly` (to load model), `DynamoDBFullAccess` (to save records), and `SNSPublish` (to send alerts).\n",
    "\n",
    "### Summary of Code Migration\n",
    "\n",
    "| Notebook Source | AWS Destination | Action |\n",
    "| :--- | :--- | :--- |\n",
    "| **NB1, NB2, NB3** (EDA/Cleaning) | **N/A (One-off)** | These are for your understanding. The *cleaning logic* (e.g., dropping IDs) must be moved into the `train.py` script. |\n",
    "| **NB6** (Pipeline Definition) | **EC2 `train.py`** | Copy the `Pipeline(...)` definition exactly. This ensures your training and inference match. |\n",
    "| **NB6** (Model Saving) | **EC2 `train.py`** | Add `joblib.dump` to save the file to S3. |\n",
    "| **NB4** (Prediction Logic) | **Lambda (Predictor)** | The `clf.predict()` line goes here. |\n",
    "| **New Logic** | **Lambda (Retraining)** | Logic to pull data from DynamoDB and re-run `pipeline.fit()`. |\n",
    "\n",
    "### How to start today?\n",
    "1.  **Local Test:** Create the `train.py` script on your laptop. Run it. Ensure it produces a `.pkl` file.\n",
    "2.  **Local Inference:** Write a small script that loads that `.pkl` file and predicts on a dummy array of 30 numbers.\n",
    "3.  **AWS Upload:** Once those two work locally, spin up the EC2 and move `train.py` there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e40cff",
   "metadata": {},
   "source": [
    "### Phase 3: The Retraining Loop (EventBridge + Lambda)\n",
    "\n",
    "This is the \"AWS Lambda: Retraining\" in your diagram.\n",
    "\n",
    "**How to adjust data location:**\n",
    "In your notebooks, you read from CSV. In this Lambda, you must read from **DynamoDB** (where you are storing new patient records) or a \"Historical Data\" folder in **S3**.\n",
    "\n",
    "**Logic:**\n",
    "1.  Triggered by EventBridge (e.g., every Sunday).\n",
    "2.  Fetch historical data + new \"Confirmed\" data from DynamoDB.\n",
    "3.  Combine them into a DataFrame.\n",
    "4.  Run the same training logic as Phase 1.\n",
    "5.  Overwrite `latest_model.pkl` in S3.\n",
    "\n",
    "*Note: If your dataset grows large, Lambda might time out (15 min limit). If that happens, you will need to move this logic back to EC2 or AWS SageMaker, but for < 10,000 rows, Lambda is fine.*\n",
    "\n",
    "### Phase 4: Infrastructure Setup Checklist\n",
    "\n",
    "1.  **S3 Buckets:**\n",
    "    *   `bucket-models`: To store `latest_model.pkl`.\n",
    "    *   `bucket-website`: To host your HTML/JS frontend (if applicable).\n",
    "\n",
    "2.  **DynamoDB Table:**\n",
    "    *   Partition Key: `patient_id` (String).\n",
    "    *   Attributes: `prediction`, `features`, `actual_diagnosis` (for the feedback loop).\n",
    "\n",
    "3.  **SNS Topic:**\n",
    "    *   Create a topic.\n",
    "    *   Subscribe your email address to it.\n",
    "\n",
    "4.  **IAM Roles (The Glue):**\n",
    "    *   **EC2 Role:** Needs `S3FullAccess` (to upload the model).\n",
    "    *   **Lambda Role:** Needs `S3ReadOnly` (to load model), `DynamoDBFullAccess` (to save records), and `SNSPublish` (to send alerts).\n",
    "\n",
    "### Summary of Code Migration\n",
    "\n",
    "| Notebook Source | AWS Destination | Action |\n",
    "| :--- | :--- | :--- |\n",
    "| **NB1, NB2, NB3** (EDA/Cleaning) | **N/A (One-off)** | These are for your understanding. The *cleaning logic* (e.g., dropping IDs) must be moved into the `train.py` script. |\n",
    "| **NB6** (Pipeline Definition) | **EC2 `train.py`** | Copy the `Pipeline(...)` definition exactly. This ensures your training and inference match. |\n",
    "| **NB6** (Model Saving) | **EC2 `train.py`** | Add `joblib.dump` to save the file to S3. |\n",
    "| **NB4** (Prediction Logic) | **Lambda (Predictor)** | The `clf.predict()` line goes here. |\n",
    "| **New Logic** | **Lambda (Retraining)** | Logic to pull data from DynamoDB and re-run `pipeline.fit()`. |\n",
    "\n",
    "### How to start today?\n",
    "1.  **Local Test:** Create the `train.py` script on your laptop. Run it. Ensure it produces a `.pkl` file.\n",
    "2.  **Local Inference:** Write a small script that loads that `.pkl` file and predicts on a dummy array of 30 numbers.\n",
    "3.  **AWS Upload:** Once those two work locally, spin up the EC2 and move `train.py` there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c237ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize clients\n",
    "s3 = boto3.client('s3')\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "table = dynamodb.Table('YOUR_DYNAMODB_TABLE_NAME')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    # 1. Fetch Data from DynamoDB (New Data)\n",
    "    # Scan is expensive, in production use Query or export to S3\n",
    "    response = table.scan()\n",
    "    items = response['Items']\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    new_data = pd.DataFrame(items)\n",
    "    \n",
    "    # 2. Fetch Historical Data (from S3)\n",
    "    s3.download_file('YOUR_DATA_BUCKET', 'historical_data.csv', '/tmp/historical.csv')\n",
    "    historical_data = pd.read_csv('/tmp/historical.csv')\n",
    "    \n",
    "    # 3. Combine Data\n",
    "    # Ensure columns match. You might need to parse 'features' string back to columns\n",
    "    # This is a simplified example\n",
    "    # full_data = pd.concat([historical_data, new_data])\n",
    "    \n",
    "    # For this example, we'll just retrain on historical to show the logic\n",
    "    X = historical_data.iloc[:, 1:31].values\n",
    "    y = historical_data.iloc[:, 0].values\n",
    "    \n",
    "    # 4. Retrain Model\n",
    "    pipeline = Pipeline([\n",
    "        ('scl', StandardScaler()),\n",
    "        ('pca', PCA(n_components=2)),\n",
    "        ('clf', SVC(C=100.0, kernel='rbf', probability=True)) \n",
    "    ])\n",
    "    pipeline.fit(X, y)\n",
    "    \n",
    "    # 5. Save and Upload New Model\n",
    "    joblib.dump(pipeline, '/tmp/model_v2.pkl')\n",
    "    s3.upload_file('/tmp/model_v2.pkl', 'YOUR_MODEL_BUCKET_NAME', 'latest_model.pkl')\n",
    "    \n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': 'Model Retrained and Updated Successfully'\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "breastcancer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
