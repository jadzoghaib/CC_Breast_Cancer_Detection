{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95d7471f",
   "metadata": {},
   "source": [
    "# AWS Migration: Step-by-Step Implementation Guide\n",
    "\n",
    "This notebook serves as a checklist and guide for migrating your Breast Cancer Prediction model to AWS. Follow these steps sequentially.\n",
    "\n",
    "## Prerequisites\n",
    "*   An active AWS Account.\n",
    "*   The `breastcancer_env.yaml` file on your local machine.\n",
    "*   The `clean-data.csv` file on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb303726",
   "metadata": {},
   "source": [
    "## Step 1: Create S3 Buckets (Storage)\n",
    "1.  Log in to the **AWS Console** and search for **S3**.\n",
    "2.  Click **Create bucket**.\n",
    "3.  **Bucket 1 (Data):** Name it something unique (e.g., `breast-cancer-data-jad-2025`).\n",
    "    *   Keep defaults (Block Public Access: On).\n",
    "    *   Click **Create bucket**.\n",
    "4.  **Bucket 2 (Models):** Name it something unique (e.g., `breast-cancer-models-jad-2025`).\n",
    "    *   Click **Create bucket**.\n",
    "5.  **Upload Data:**\n",
    "    *   Go into your **Data Bucket**.\n",
    "    *   Click **Upload** -> **Add files**.\n",
    "    *   Select your local `data/clean-data.csv`.\n",
    "    *   Click **Upload**.\n",
    "\n",
    "## Step 2: Create IAM Role (Permissions)\n",
    "This role allows your EC2 instance to write to your S3 buckets.\n",
    "1.  Search for **IAM** in the console.\n",
    "2.  Go to **Roles** -> **Create role**.\n",
    "3.  **Trusted entity type:** AWS Service.\n",
    "4.  **Service or use case:** EC2.\n",
    "5.  Click **Next**.\n",
    "6.  **Add permissions:** Search for and select `AmazonS3FullAccess`.\n",
    "    *   *Note: In a strict production environment, you would limit this to just your specific buckets, but FullAccess is fine for setup.*\n",
    "7.  Click **Next**.\n",
    "8.  **Role name:** `EC2-S3-Access-Role`.\n",
    "9.  Click **Create role**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33e7899",
   "metadata": {},
   "source": [
    "## Step 3: Launch EC2 Instance (Compute)\n",
    "1.  Search for **EC2** in the console.\n",
    "2.  Click **Launch Instance**.\n",
    "3.  **Name:** `Breast-Cancer-Training-Server`.\n",
    "4.  **OS Images:** Select **Ubuntu** (Ubuntu Server 24.04 LTS is good).\n",
    "5.  **Instance Type:** `t3.medium` (2 vCPU, 4GB RAM) is recommended for this workload. `t2.micro` might run out of memory during installation.\n",
    "6.  **Key pair:**\n",
    "    *   Click **Create new key pair**.\n",
    "    *   Name: `breast-cancer-key`.\n",
    "    *   Type: `.pem` (for OpenSSH/Mac/Linux) or `.ppk` (for PuTTY/Windows).\n",
    "    *   **Download Key Pair** (Keep this safe!).\n",
    "7.  **Network settings:** Check the boxes for \"Allow SSH traffic from\". Select \"My IP\" for security.\n",
    "8.  **Advanced details (Crucial Step):**\n",
    "    *   Find **IAM instance profile**.\n",
    "    *   Select the role you created in Step 2: `EC2-S3-Access-Role`.\n",
    "9.  Click **Launch instance**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ebb7c",
   "metadata": {},
   "source": [
    "## Step 4: Create the Training Script\n",
    "Create a file named `train.py` on your local machine with the following content. \n",
    "**Important:** Replace `YOUR_SOURCE_DATA_BUCKET_NAME` and `YOUR_MODEL_BUCKET_NAME` with the actual names of the buckets you created in Step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ad451b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# 1. UPDATE THESE WITH YOUR ACTUAL BUCKET NAMES\n",
    "SOURCE_BUCKET = 'breast-cancer-cleaneddata' \n",
    "MODEL_BUCKET = 'breast-cancer-prediction-models'\n",
    "DATA_FILE = 'clean-data.csv'\n",
    "LOCAL_DATA_PATH = 'data/clean-data.csv'\n",
    "\n",
    "# 1. Load Data\n",
    "if not os.path.exists(LOCAL_DATA_PATH):\n",
    "    os.makedirs('data', exist_ok=True)\n",
    "    print(f\"Downloading {DATA_FILE} from S3...\")\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        s3.download_file(SOURCE_BUCKET, DATA_FILE, LOCAL_DATA_PATH)\n",
    "        print(\"Download successful.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading from S3: {e}\")\n",
    "        print(\"Make sure you updated SOURCE_BUCKET and that your EC2 role has S3 permissions.\")\n",
    "        raise\n",
    "\n",
    "data = pd.read_csv(LOCAL_DATA_PATH)\n",
    "\n",
    "# 2. Preprocessing (Cleaning)\n",
    "if 'Unnamed: 0' in data.columns: data.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "if 'id' in data.columns: data.drop('id', axis=1, inplace=True)\n",
    "if 'Unnamed: 32' in data.columns: data.drop('Unnamed: 32', axis=1, inplace=True)\n",
    "\n",
    "X = data.iloc[:, 1:].values\n",
    "y = data.iloc[:, 0].values\n",
    "\n",
    "# Encode Target\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# 3. Define Pipeline (StandardScaler -> PCA -> SVM)\n",
    "pipeline = Pipeline([\n",
    "    ('scl', StandardScaler()),\n",
    "    ('pca', PCA(n_components=2)),\n",
    "    # Best params from Notebook (C=0.1, gamma=0.001, kernel='linear')\n",
    "    ('clf', SVC(C=0.1, gamma=0.001, kernel='linear', probability=True)) \n",
    "])\n",
    "\n",
    "# 4. Train\n",
    "print(\"Training model...\")\n",
    "pipeline.fit(X, y)\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# 5. Save and Upload\n",
    "LOCAL_MODEL_PATH = 'model_v1.pkl'\n",
    "joblib.dump(pipeline, LOCAL_MODEL_PATH)\n",
    "print(f\"Model saved locally to {LOCAL_MODEL_PATH}\")\n",
    "\n",
    "print(\"Uploading to S3...\")\n",
    "s3 = boto3.client('s3')\n",
    "s3.upload_file(LOCAL_MODEL_PATH, MODEL_BUCKET, 'latest_model.pkl')\n",
    "print(f\"Success! Model uploaded to s3://{MODEL_BUCKET}/latest_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a079c3b",
   "metadata": {},
   "source": [
    "### Is this the most efficient model?\n",
    "\n",
    "**Short Answer:** Yes, this architecture (SVM with Scaling) was the top performer in your analysis. We have updated the script with your specific hyperparameters (`C=0.1`, `kernel='linear'`) based on your Grid Search results.\n",
    "\n",
    "**Detailed Explanation:**\n",
    "1.  **Why SVM?**\n",
    "    In your notebook `NB6 Comparison between different classifiers`, you compared Logistic Regression, LDA, KNN, CART, Naive Bayes, and SVM.\n",
    "    *   Initially, SVM performed poorly.\n",
    "    *   **However**, after you applied `StandardScaler` (Standardization), SVM became the **most accurate algorithm**, outperforming the others. This is why we selected `SVC` for your production script.\n",
    "\n",
    "2.  **Why PCA?**\n",
    "    Your notebooks (`NB5` and `NB6`) use Principal Component Analysis (PCA).\n",
    "    *   In `NB6`, you explicitly defined a pipeline: `StandardScaler` -> `PCA(n_components=2)` -> `SVC`.\n",
    "    *   Using `n_components=2` makes the model extremely **efficient** (fast to train, small to store) because it only looks at the 2 most important variance features instead of all 30.\n",
    "    *   *Trade-off:* If you want slightly higher accuracy at the cost of speed, you could increase `n_components` to 10 (as explored in `NB5`) or remove PCA entirely. But for a \"lightweight\" cloud deployment, keeping PCA is a smart move for efficiency.\n",
    "\n",
    "3.  **Why C=0.1?**\n",
    "    In `NB5`, you ran a `GridSearchCV` to find the best `C` (Regularization) and `gamma`.\n",
    "    *   We have updated the `train.py` script to use `C=0.1` and `kernel='linear'` as per your specific results.\n",
    "\n",
    "**Conclusion:**\n",
    "The `train.py` script represents the **best architecture** (Scaled SVM) found in your research. It is \"efficient\" because it balances high accuracy (via SVM) with low computational cost (via PCA)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e4a85b",
   "metadata": {},
   "source": [
    "## Step 5: Deploy to EC2\n",
    "Now that your infrastructure is ready and your code is written, let's run it.\n",
    "\n",
    "1.  **Connect to EC2:**\n",
    "    *   Open your terminal (WSL/Ubuntu).\n",
    "    *   Run the following command:\n",
    "    ```bash\n",
    "    ssh -i \"~/breast-cancer-key.pem\" ubuntu@18.184.78.242\n",
    "    ```\n",
    "\n",
    "2.  **Install Environment Manager (Micromamba):**\n",
    "    *(You have already completed this step)*\n",
    "    Run these commands inside the EC2 terminal:\n",
    "    ```bash\n",
    "    \"${SHELL}\" <(curl -L micro.mamba.pm/install.sh)\n",
    "    # Press Enter to accept defaults\n",
    "    source ~/.bashrc\n",
    "    ```\n",
    "\n",
    "3.  **Upload Files:**\n",
    "    **CRITICAL:** Do NOT run this in the EC2 terminal. Open a **NEW** terminal window (WSL/Ubuntu) on your local machine.\n",
    "    \n",
    "    Use `scp` to copy your files to the server.\n",
    "    ```bash\n",
    "    # 1. Upload Environment File\n",
    "    scp -i \"~/breast-cancer-key.pem\" \"/mnt/c/Users/Jad Zoghaib/OneDrive/Desktop/CC_Breast_Cancer/Breast-cancer-risk-prediction/breastcancer_env.yaml\" ubuntu@18.184.78.242:~/\n",
    "    \n",
    "    # 2. Upload Training Script\n",
    "    scp -i \"~/breast-cancer-key.pem\" \"/mnt/c/Users/Jad Zoghaib/OneDrive/Desktop/CC_Breast_Cancer/Breast-cancer-risk-prediction/train.py\" ubuntu@18.184.78.242:~/\n",
    "    ```\n",
    "\n",
    "4.  **Setup & Run (Back in EC2 Terminal):**\n",
    "    ```bash\n",
    "    # Create environment\n",
    "    micromamba create -f breastcancer_env.yaml\n",
    "    \n",
    "    # Activate\n",
    "    micromamba activate breastcancer\n",
    "    \n",
    "    # Install boto3 (if missing)\n",
    "    micromamba install boto3\n",
    "    \n",
    "    # Run the training\n",
    "    python train.py\n",
    "    ```\n",
    "\n",
    "5.  **Verify:**\n",
    "    Check your S3 bucket \"Models\". You should see `latest_model.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e2b1c4",
   "metadata": {},
   "source": [
    "#  ðŸš¨ðŸš¨ðŸš¨ Part that is not done  ðŸš¨ðŸš¨ðŸš¨\n",
    " \n",
    "## Phase 1: The Base Application (Inference)\n",
    "Now that we have a trained model on S3, we will build the core application: a website where doctors can upload data and get a prediction.\n",
    "\n",
    "### Step 6: Create the Predictor Lambda\n",
    "**What is this?**\n",
    "AWS Lambda is a \"serverless\" compute service. It allows us to run our Python prediction code without managing a server (like EC2).\n",
    "\n",
    "**How it fits:**\n",
    "This function acts as the **Brain** of the application. When triggered, it will:\n",
    "1.  Download the `latest_model.pkl` from your S3 bucket.\n",
    "2.  Receive the patient data (30 features) from the website.\n",
    "3.  Use the model to calculate a prediction (Benign/Malignant).\n",
    "4.  Return the result.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Search for **Lambda** in AWS Console.\n",
    "2.  Click **Create function**.\n",
    "3.  **Name:** `BreastCancerPredictor`.\n",
    "4.  **Runtime:** **Python 3.10**.\n",
    "5.  **Permissions:**\n",
    "    *   Click \"Change default execution role\".\n",
    "    *   Select \"Create a new role from AWS policy templates\".\n",
    "    *   Role name: `Lambda-ML-Role`.\n",
    "    *   **Policy templates:** Search for \"S3 object read-only permissions\".\n",
    "6.  Click **Create function**.\n",
    "\n",
    "**Add Dependencies (Layers):**\n",
    "1.  Scroll to the **Layers** section at the bottom.\n",
    "2.  Click **Add a layer**.\n",
    "3.  Choose **AWS layers**.\n",
    "4.  Select **AWSSDKPandas-Python310** (This includes pandas and numpy).\n",
    "5.  Click **Add**.\n",
    "\n",
    "**Lambda Code:**\n",
    "Paste this into the code editor and click **Deploy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af36860",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import boto3\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BUCKET_NAME = 'breast-cancer-prediction-models'\n",
    "MODEL_KEY = 'latest_model.pkl'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "model = None\n",
    "\n",
    "def load_model():\n",
    "    global model\n",
    "    if model is None:\n",
    "        with BytesIO() as data:\n",
    "            s3.download_fileobj(BUCKET_NAME, MODEL_KEY, data)\n",
    "            data.seek(0)\n",
    "            model = joblib.load(data)\n",
    "    return model\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        body = json.loads(event['body'])\n",
    "        features = body.get('features') # List of 30 numbers\n",
    "        \n",
    "        clf = load_model()\n",
    "        prediction = clf.predict([features])[0] # 0 or 1\n",
    "        result = 'Malignant' if prediction == 1 else 'Benign'\n",
    "        \n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'headers': {\n",
    "                'Access-Control-Allow-Origin': '*',\n",
    "                'Access-Control-Allow-Headers': 'Content-Type',\n",
    "                'Access-Control-Allow-Methods': 'OPTIONS,POST'\n",
    "            },\n",
    "            'body': json.dumps({'prediction': result})\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {'statusCode': 400, 'body': json.dumps(str(e))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff1345",
   "metadata": {},
   "source": [
    "### Step 7: Setup API Gateway\n",
    "**What is this?**\n",
    "API Gateway is a service that creates and publishes secure APIs.\n",
    "\n",
    "**How it fits:**\n",
    "Your Lambda function is hidden inside your private AWS cloud. The API Gateway acts as the **Front Door**. It provides a public URL (endpoint) that your HTML website can send data to. When the API Gateway receives data, it passes it to the Lambda function.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Search for **API Gateway**.\n",
    "2.  Click **Create API** -> **REST API** (Build).\n",
    "3.  **Name:** `BreastCancerAPI`. Click **Create API**.\n",
    "4.  **Create Resource:** Actions -> Create Resource -> Name: `predict` -> Check **Enable API Gateway CORS** -> Create.\n",
    "5.  **Create Method:** Select `/predict` -> Actions -> Create Method -> **POST**.\n",
    "    *   Integration type: **Lambda Function**.\n",
    "    *   Function: `BreastCancerPredictor`.\n",
    "    *   Save -> OK.\n",
    "6.  **Deploy:** Actions -> Deploy API -> Stage: `prod` -> Deploy.\n",
    "7.  **Copy URL:** Save the **Invoke URL** (e.g., `https://xyz.../prod`).\n",
    "\n",
    "### Step 8: Create the Frontend (V1)\n",
    "**What is this?**\n",
    "This is the user interface (UI) for the doctor. It is a simple HTML/JavaScript file.\n",
    "\n",
    "**How it fits:**\n",
    "This file will be hosted on S3 (acting as a web server). It runs in the doctor's browser, reads the CSV file they select, and sends the data to the **API Gateway** URL we just created. It then displays the result returned by the Lambda.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create `index.html` locally.\n",
    "2.  Paste the code below (Update `API_URL` with your link).\n",
    "3.  Upload to your S3 Data Bucket and enable **Static website hosting** in Properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c787bb9",
   "metadata": {
    "vscode": {
     "languageId": "html"
    }
   },
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Breast Cancer Risk Predictor</title>\n",
    "    <style>\n",
    "        body { font-family: sans-serif; max-width: 800px; margin: 0 auto; padding: 20px; }\n",
    "        .container { border: 1px solid #ccc; padding: 20px; border-radius: 8px; }\n",
    "        .hidden { display: none; }\n",
    "        .malignant { color: red; font-weight: bold; }\n",
    "        .benign { color: green; font-weight: bold; }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"container\">\n",
    "        <h1>Breast Cancer Risk Assessment</h1>\n",
    "        <p>Upload patient data (CSV with 30 features).</p>\n",
    "        \n",
    "        <label>Patient ID: <input type=\"text\" id=\"patientId\"></label><br><br>\n",
    "        <input type=\"file\" id=\"csvFile\" accept=\".csv\"><br><br>\n",
    "        <button onclick=\"analyze()\">Analyze Risk</button>\n",
    "        \n",
    "        <div id=\"resultArea\" class=\"hidden\">\n",
    "            <h3>Prediction: <span id=\"predictionText\"></span></h3>\n",
    "            <hr>\n",
    "            <h4>Doctor's Validation</h4>\n",
    "            <p>Please confirm the final diagnosis to update the medical database.</p>\n",
    "            <select id=\"finalDiagnosis\">\n",
    "                <option value=\"Benign\">Confirmed Benign</option>\n",
    "                <option value=\"Malignant\">Confirmed Malignant</option>\n",
    "            </select>\n",
    "            <button onclick=\"submitData()\">Submit to Database</button>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "    <script>\n",
    "        const PREDICT_URL = 'https://YOUR-API-ID.execute-api.us-east-1.amazonaws.com/prod/predict';\n",
    "        const SAVE_URL = 'https://YOUR-API-ID.execute-api.us-east-1.amazonaws.com/prod/save'; // We will create this next\n",
    "        \n",
    "        let currentFeatures = [];\n",
    "\n",
    "        function analyze() {\n",
    "            const file = document.getElementById('csvFile').files[0];\n",
    "            const reader = new FileReader();\n",
    "            reader.onload = function(e) {\n",
    "                currentFeatures = e.target.result.trim().split(',').map(Number);\n",
    "                \n",
    "                fetch(PREDICT_URL, {\n",
    "                    method: 'POST',\n",
    "                    body: JSON.stringify({ features: currentFeatures })\n",
    "                })\n",
    "                .then(res => res.json())\n",
    "                .then(data => {\n",
    "                    const pred = data.prediction;\n",
    "                    const span = document.getElementById('predictionText');\n",
    "                    span.innerText = pred;\n",
    "                    span.className = pred === 'Malignant' ? 'malignant' : 'benign';\n",
    "                    \n",
    "                    // Auto-select the predicted value in dropdown\n",
    "                    document.getElementById('finalDiagnosis').value = pred;\n",
    "                    document.getElementById('resultArea').classList.remove('hidden');\n",
    "                });\n",
    "            };\n",
    "            reader.readAsText(file);\n",
    "        }\n",
    "\n",
    "        function submitData() {\n",
    "            const patientId = document.getElementById('patientId').value;\n",
    "            const diagnosis = document.getElementById('finalDiagnosis').value;\n",
    "            \n",
    "            fetch(SAVE_URL, {\n",
    "                method: 'POST',\n",
    "                body: JSON.stringify({\n",
    "                    patient_id: patientId,\n",
    "                    features: currentFeatures,\n",
    "                    diagnosis: diagnosis\n",
    "                })\n",
    "            })\n",
    "            .then(res => res.json())\n",
    "            .then(data => alert('Data saved to database successfully!'));\n",
    "        }\n",
    "    </script>\n",
    "</body>\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1acb16",
   "metadata": {},
   "source": [
    "## Phase 2: Notifications (Alerts)\n",
    "We want to send an email if the prediction is Malignant.\n",
    "\n",
    "### Step 9: Create SNS Topic\n",
    "1.  Go to **SNS** -> **Create Topic** -> Name: `BreastCancerAlerts` -> Standard.\n",
    "2.  **Create Subscription** -> Protocol: Email -> Endpoint: Your Email.\n",
    "3.  Confirm the subscription in your email.\n",
    "4.  Copy the **Topic ARN**.\n",
    "\n",
    "### Step 10: Update Predictor Lambda\n",
    "1.  Go back to your `BreastCancerPredictor` Lambda.\n",
    "2.  Add `sns:Publish` permissions to its IAM Role.\n",
    "3.  Update the code to send an alert if `result == 'Malignant'`.\n",
    "\n",
    "## Phase 3: The Feedback Loop (Data Collection)\n",
    "We will create a separate flow to save validated data to DynamoDB.\n",
    "\n",
    "### Step 11: Create DynamoDB Table\n",
    "1.  Go to **DynamoDB** -> **Create table**.\n",
    "2.  Name: `BreastCancerPredictions`.\n",
    "3.  Partition key: `patient_id` (String).\n",
    "4.  Sort key: `timestamp` (String).\n",
    "\n",
    "### Step 12: Create \"SaveData\" Lambda\n",
    "This function handles the \"Submit to Database\" button.\n",
    "\n",
    "1.  Create a new Lambda: `BreastCancerSaveData`.\n",
    "2.  Add permissions for **DynamoDB** (Write).\n",
    "3.  **Code:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aef0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "dynamodb = boto3.resource('dynamodb')\n",
    "TABLE_NAME = 'BreastCancerPredictions'\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    try:\n",
    "        body = json.loads(event['body'])\n",
    "        table = dynamodb.Table(TABLE_NAME)\n",
    "        \n",
    "        item = {\n",
    "            'patient_id': body['patient_id'],\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'features': str(body['features']),\n",
    "            'diagnosis': body['diagnosis'] # This is the CONFIRMED diagnosis from the doctor\n",
    "        }\n",
    "        table.put_item(Item=item)\n",
    "        \n",
    "        return {'statusCode': 200, 'body': json.dumps('Saved')}\n",
    "    except Exception as e:\n",
    "        return {'statusCode': 400, 'body': json.dumps(str(e))}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0085ad",
   "metadata": {},
   "source": [
    "### Step 13: Update API Gateway\n",
    "1.  Go to `BreastCancerAPI`.\n",
    "2.  Create a new Resource: `save`.\n",
    "3.  Create Method: **POST** -> Integration: `BreastCancerSaveData`.\n",
    "4.  **Deploy API** again to `prod`.\n",
    "5.  **Update your HTML file:** Replace `SAVE_URL` with this new endpoint.\n",
    "\n",
    "## Phase 4: Continuous Learning (Retraining)\n",
    "Every 2 days, we move data from DynamoDB to S3 and retrain.\n",
    "\n",
    "### Step 14: Automate Retraining\n",
    "1.  **Create `sync_data.py` on EC2:**\n",
    "    (Use the script provided in the previous version of this guide, but ensure it maps the DynamoDB `diagnosis` field to the CSV target column).\n",
    "\n",
    "2.  **Setup Cron Job:**\n",
    "    Run `crontab -e` on EC2 and add:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d970304",
   "metadata": {
    "vscode": {
     "languageId": "bash"
    }
   },
   "outputs": [],
   "source": [
    "0 0 */2 * * /home/ubuntu/micromamba/envs/breastcancer/bin/python /home/ubuntu/sync_data.py && /home/ubuntu/micromamba/envs/breastcancer/bin/python /home/ubuntu/train.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
